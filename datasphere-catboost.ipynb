{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python"},"papermill":{"duration":392.708184,"end_time":"2020-11-23T16:52:33.291774","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-11-23T16:46:00.583590","version":"2.1.0"},"notebookId":"ad948183-ff32-4aea-9319-3201ee2851bc"},"cells":[{"cell_type":"code","source":"#!M\n# !pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1\n# %pip install --upgrade pip\n# %pip install -r /home/jupyter/work/resources/riiidNew/requirements.txt --upgrade\n# %pip install scipy  --ignore-installed scipy --upgrade\nprint(\"ok\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":30.191831,"end_time":"2020-11-23T16:46:34.873724","exception":false,"start_time":"2020-11-23T16:46:04.681893","status":"completed"},"tags":[],"cellId":"r4gbiatk8h938d934p4aip","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# if str(Path.home()) == \"/root\":  # DATASPHERE\nimport sys\nimport gc\nfrom pathlib import Path\nsys.path.append(str(Path.cwd()/'riiidNew'))\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport pyarrow\nfrom collections import defaultdict\nfrom catboost import CatBoostClassifier, Pool\nfrom preprocess import preprocess_train_data\nfrom catboost_bayesian_search import bayesian_catboost_searchCV, param_adjust_dtypes\nimport config\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')\n\n\nif str(Path.home()) == \"/home/sergey\":   # Home computer\n    kaggle_path = Path.cwd()/\"kaggle_tmp/\"\n    questions_df = pd.read_csv('/mnt/data30G/2020riiid/questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n    test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid_1e4.feather\", columns=dtypes.keys())\n    train_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train_1e5.feather\", columns=dtypes.keys())\n    # test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid_1e4.feather\", columns=dtypes.keys())\n    # train_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train.feather\", columns=dtypes.keys())\n    # test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid.feather\", columns=dtypes.keys())\n\nelif str(Path.home()) == \"/root\":   # DATASPHERE\n    \n    path = Path.cwd()/\"riiidNew\"/\"data1\"\n    kaggle_path = Path.cwd()/\"riiidNew\"/\"kaggle\"\n    questions_df = pd.read_csv(path/'questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n    train_df = pd.read_feather(path/\"train.feather\", columns=config.dtypes.keys())\n    # train_df = pd.read_feather(path/\"train_5e6.feather\", columns=config.dtypes.keys())\n    # train_df = pd.read_feather(path/\"train_1e5.feather\", columns=config.dtypes.keys())\n    # train_df = pd.read_pickle(path/'cv2_train.pickle.zip').astype(config.dtypes, errors=\"ignore\")\n    # test_df = pd.read_pickle(path/\"cv2_valid.pickle.zip\").astype(config.dtypes, errors=\"ignore\")\n\n# print(f\"train_df shape = {train_df.shape}\")\n\n# Preprocess for CV\ntrain_user_agg, train_content_agg, train_df = preprocess_train_data(train_df, questions_df, config.target, config.dtypes)\n# test_user_agg, test_content_agg, test_df = preprocess_train_data(test_df, questions_df, config.target, config.dtypes)\nprint(\"Preprocess for CV ok\")\n\n# Training and validating data\ntrain_set = Pool(train_df[config.features], label = train_df[config.target], cat_features=config.cat_features)\n# val_set = Pool(test_df[features], label = test_df[target])\n\nprint(\"optimizer_maxCV ..\")\noptimizer_maxCV = bayesian_catboost_searchCV(\n    train_set,\n    prior_params=config.prior_params,\n    pds=config.pds, pds_dtypes=config.pds_dtypes,\n    init_points=3, n_iter=25, verbose=False,\n)\nprint(\"optimizer_maxCV ..\")\nprint(optimizer_maxCV)\n\nparams = param_adjust_dtypes(config.prior_params, \n                             config.pds_dtypes, \n                             optimizer_maxCV[\"params\"])\nprint(\"params\")\nprint(params)\n\n# pds_fitted = {key: config.pds_dtypes[key](val) for key, val in optimizer_maxCV[\"params\"].items()}\n# params = prior_params.copy()\n# params.update(pds_fitted)\n\n# Ratio is 6/24 = 25%\n# valid_df = train_df.groupby('user_id').tail(6)\n# train_df.drop(valid_df.index, inplace = True)\n\nimport pickle\n\nuser_sum_dict = train_user_agg['sum'].astype('int32').to_dict(defaultdict(int))\nuser_count_dict = train_user_agg['count'].astype('int32').to_dict(defaultdict(int))\ncontent_sum_dict = train_content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = train_content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\nfor filename, dic in zip([\"user_sum_dict\", \"user_count_dict\", \"content_sum_dict\", \"content_count_dict\"],\n                         [user_sum_dict, user_count_dict, content_sum_dict, content_count_dict]):\n    with open(f'{kaggle_path}/{filename}.pickle', 'wb') as handle:\n        pickle.dump(dic, handle)\n\n# del train_df, user_sum_dict, train_user_agg, user_count_dict\n# del content_sum_dict, train_content_agg, content_count_dict\n# gc.collect()\n\n# Obtain new trai_df and validation test_df\n# train_df = pd.read_feather(path/\"cv2_train.feather\", columns=config.dtypes.keys())\n\n# train_df = pd.read_pickle(path/'cv2_train.pickle.zip').astype(config.dtypes, errors=\"ignore\").reset_index()\n# train_df = train_df.iloc[:int(1e5)]\n\n# test_df = pd.read_feather(path/\"cv2_test.feather\", columns=config.dtypes.keys())\n# test_df = pd.read_pickle(path/\"cv2_valid.pickle.zip\").astype(config.dtypes, errors=\"ignore\").reset_index()\n# test_df = test_df.iloc[:int(1e4)]\n\nprint(\"cv2_train.feather, cv2_test.feather ok!\") \n\n\n# Preprocess for model fitting\n# train_user_agg, train_content_agg, train_df = preprocess_train_data(train_df, questions_df, config.target, config.dtypes)\n# test_user_agg, test_content_agg, test_df = preprocess_train_data(test_df, questions_df, config.target, config.dtypes)\nprint(\"Preprocess for model fitting ok\")\n\n# Ratio is 6/24 = 25%\ntest_df = train_df.groupby('user_id').tail(6)\ntrain_df.drop(test_df.index, inplace = True)\n\n# Training and validating data\ntrain_set = Pool(train_df[config.features], label = train_df[config.target], cat_features=config.cat_features)\nval_set = Pool(test_df[config.features], label = test_df[config.target], cat_features=config.cat_features)\n\n\n# val_set = Pool(valid_df[features], label = valid_df[target])\n\n# Model definition\n# params = {\n#     'loss_function': 'Logloss', \n#     'eval_metric': 'AUC', \n#     'custom_metric': 'AUC:hints=skip_train~false', \n#     'task_type': 'GPU', 'grow_policy': 'Lossguide', \n#     'iterations': 2000, 'learning_rate': 0.03, \n#     'random_seed': 0, 'bootstrap_type': 'Bayesian', \n#     'l2_leaf_reg': 463.75241951019393, \n#     'depth': 23, 'max_leaves': 21, \n#     'border_count': 163, \n#     'verbose': 250, \n#     'od_type': 'Iter', \n#     'od_wait': 50, \n#     'bagging_temperature': 2.055885684521886}\nparams[\"learning_rate\"] = 4e-3\nparams['iterations'] = 20000\n\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set, eval_set = val_set, use_best_model = True)\nprint(f\"kaggle_path {kaggle_path}\")\n\nmodel.save_model(f\"{kaggle_path/'catboost.model'}\")\n\nprint(\"model.get_best_score()\")\nprint(model.get_best_score())","metadata":{"pycharm":{"name":"#%%\n"},"cellId":"x457vefy7xyyyibor5o8f","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Preprocess for CV ok\noptimizer_maxCV ..\n0:\tlearn: 0.7394486\ttest: 0.7393291\tbest: 0.7393291 (0)\n250:\tlearn: 0.7480574\ttest: 0.7477063\tbest: 0.7477063 (250)\n500:\tlearn: 0.7485925\ttest: 0.7480717\tbest: 0.7480717 (500)\n750:\tlearn: 0.7489159\ttest: 0.7482441\tbest: 0.7482441 (750)\n1000:\tlearn: 0.7491736\ttest: 0.7483632\tbest: 0.7483632 (1000)\n1250:\tlearn: 0.7494008\ttest: 0.7484592\tbest: 0.7484592 (1250)\n1500:\tlearn: 0.7495914\ttest: 0.7485240\tbest: 0.7485240 (1500)\n1750:\tlearn: 0.7497675\ttest: 0.7485799\tbest: 0.7485799 (1750)\n1999:\tlearn: 0.7499298\ttest: 0.7486264\tbest: 0.7486264 (1999)\ttotal: 9m 27s\tremaining: 0us\n0:\tlearn: 0.7383062\ttest: 0.7382177\tbest: 0.7382177 (0)\n250:\tlearn: 0.7476609\ttest: 0.7473773\tbest: 0.7473773 (250)\n500:\tlearn: 0.7481902\ttest: 0.7477813\tbest: 0.7477813 (500)\n750:\tlearn: 0.7484849\ttest: 0.7479628\tbest: 0.7479628 (750)\n"},{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:588: UserWarning: The following variables cannot be serialized: train_set\n  warnings.warn(message)\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)","\u001B[0;32m/home/jupyter/work/pyenv/bayes_opt/target_space.py\u001B[0m in \u001B[0;36mprobe\u001B[0;34m(self, params)\u001B[0m\n\u001B[1;32m    190\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m             \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cache\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0m_hashable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mKeyError\u001B[0m: (3.335702331413284, 159.39680281567314, 23.160141013295355, 481.83864769841443, 63.68181263560888)","\nDuring handling of the above exception, another exception occurred:\n","\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)","\u001B[0;32m<ipython-input-1-2a172b9509f2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[0mprior_params\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprior_params\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[0mpds\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpds_dtypes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpds_dtypes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m     \u001B[0minit_points\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_iter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m25\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m )\n\u001B[1;32m     58\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"optimizer_maxCV ..\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/resources/riiidNew/catboost_bayesian_search.py\u001B[0m in \u001B[0;36mbayesian_catboost_searchCV\u001B[0;34m(train_set, prior_params, pds, pds_dtypes, init_points, n_iter, verbose)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0moptimizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBayesianOptimization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcatboost_hyperparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaximize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minit_points\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/pyenv/bayes_opt/bayesian_optimization.py\u001B[0m in \u001B[0;36mmaximize\u001B[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001B[0m\n\u001B[1;32m    183\u001B[0m                 \u001B[0miteration\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprobe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_probe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlazy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bounds_transformer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/pyenv/bayes_opt/bayesian_optimization.py\u001B[0m in \u001B[0;36mprobe\u001B[0;34m(self, params, lazy)\u001B[0m\n\u001B[1;32m    114\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_queue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprobe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOPTIMIZATION_STEP\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/pyenv/bayes_opt/target_space.py\u001B[0m in \u001B[0;36mprobe\u001B[0;34m(self, params)\u001B[0m\n\u001B[1;32m    192\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    193\u001B[0m             \u001B[0mparams\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_keys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 194\u001B[0;31m             \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    195\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mregister\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    196\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/resources/riiidNew/catboost_bayesian_search.py\u001B[0m in \u001B[0;36mcatboost_hyperparams\u001B[0;34m(**dict_)\u001B[0m\n\u001B[1;32m     21\u001B[0m                     \u001B[0mplot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m                     \u001B[0mtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'TimeSeries'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m                     fold_count=3)\n\u001B[0m\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscores\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"test-AUC-mean\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/pyenv/catboost/core.py\u001B[0m in \u001B[0;36mcv\u001B[0;34m(pool, params, dtrain, iterations, num_boost_round, fold_count, nfold, inverted, partition_random_seed, seed, shuffle, logging_level, stratified, as_pandas, metric_period, verbose, verbose_eval, plot, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, folds, type)\u001B[0m\n\u001B[1;32m   5323\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mlog_fixup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_get_train_dir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5324\u001B[0m         return _cv(params, pool, fold_count, inverted, partition_random_seed, shuffle, stratified,\n\u001B[0;32m-> 5325\u001B[0;31m                    as_pandas, folds, type)\n\u001B[0m\u001B[1;32m   5326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5327\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m_catboost.pyx\u001B[0m in \u001B[0;36m_catboost._cv\u001B[0;34m()\u001B[0m\n","\u001B[0;32m_catboost.pyx\u001B[0m in \u001B[0;36m_catboost._cv\u001B[0;34m()\u001B[0m\n","\u001B[0;31mKeyboardInterrupt\u001B[0m: "]}],"execution_count":1},{"cell_type":"markdown","source":"\n{'loss_function': 'Logloss', 'eval_metric': 'AUC', 'task_type': 'GPU', 'grow_policy': 'Lossguide', 'iterations': 15000, 'learning_rate': 0.01, 'random_seed': 0, 'l2_leaf_reg': 0.1, 'depth': 8, 'border_count': 128, 'verbose': 150, 'od_type': 'Iter', 'od_wait': 50}\nbestTest = 0.7371392846\n\nbestTest = 0.7370638549\nbestTest = 0.7364509702\n","metadata":{"cellId":"nl7v0tm6byjfdfngg11ydh"}},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.051177,"end_time":"2020-11-23T16:52:31.451640","exception":false,"start_time":"2020-11-23T16:52:31.400463","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"ba4wdwyypnjxlgzfd769b9"}},{"cell_type":"code","source":"#!M\nimport pickle\n\nuser_sum_dict = train_user_agg['sum'].astype('int32').to_dict(defaultdict(int))\nuser_count_dict = train_user_agg['count'].astype('int32').to_dict(defaultdict(int))\ncontent_sum_dict = train_content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = train_content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\nfor filename, dic in zip([\"user_sum_dict\", \"user_count_dict\", \"content_sum_dict\", \"content_count_dict\"],\n                         [user_sum_dict, user_count_dict, content_sum_dict, content_count_dict]):\n    with open(f'{kaggle_path}/{filename}.pickle', 'wb') as handle:\n        pickle.dump(dic, handle)\n","metadata":{"papermill":{"duration":0.618845,"end_time":"2020-11-23T16:52:32.122494","exception":false,"start_time":"2020-11-23T16:52:31.503649","status":"completed"},"tags":[],"cellId":"7ui7ur3vc7044m7d88w7h2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()\ndel train_df\ngc.collect()\n","metadata":{"cellId":"t8443l740slicr5x1cuvl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nvalidaten_flg = True\nif validaten_flg:\n    from emulator import Iter_Valid\n    iter_test = Iter_Valid(test_df,max_user=1000)\n    predicted = []\n    def set_predict(df):\n        predicted.append(df)\nelse:\n    import riiideducation\n    env = riiideducation.make_env()\n    iter_test = env.iter_test()\n    set_predict = env.predict","metadata":{"cellId":"6qrpfrl3bhn281e5c5twea","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cumcount = sum([len(df) for df in predicted])\n# count = 0\n# pbar = tqdm(total=cumcount)\n# previous_test_df = None\n# for (current_test, current_prediction_df) in iter_test:\n#     count+=1\n#     if previous_test_df is not None:\n#         answers = eval(current_test[\"prior_group_answers_correct\"].iloc[0])\n#         responses = eval(current_test[\"prior_group_responses\"].iloc[0])\n#         previous_test_df['answered_correctly'] = answers\n#         previous_test_df['user_answer'] = responses\n#         # your feature extraction and model training code here\n#     previous_test_df = current_test.copy()\n#     current_test = current_test[current_test.content_type_id == 0]\n#     # your prediction code here\n#     current_test['answered_correctly'] = model.predict(current_test[features])  # 0.5\n#     set_predict(current_test.loc[:,['row_id', 'answered_correctly']])\n#     pbar.update(len(current_test))\n# print(f\"count {count} {len(predicted)}\")","metadata":{"papermill":{"duration":466.435487,"end_time":"2020-11-13T14:06:20.068150","exception":false,"start_time":"2020-11-13T13:58:33.632663","status":"completed"},"tags":[],"cellId":"hkq3ce6f9zot3ymxi9x1s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprior_test_df = None\nfor (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n\n        # for user_id, content_id, answered_correctly in prior_test_df[[\"user_id\", \"content_id\", target]].values:\n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n    test_df.drop(labels=\"part\", axis=1, inplace=True)\n    test_df.content_id = test_df.content_id.astype(int)\n    \n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')    \n    user_sum = np.zeros(len(test_df), dtype = np.int32)\n    user_count = np.zeros(len(test_df), dtype = np.int32)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n\n    test_df['user_correctness'] = user_sum / user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum / content_count\n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    set_predict(test_df[['row_id', target]])","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"yzork8xo17phg263eq5lvj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!M\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"r0wtm7br3uocd9325isg1s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!M\n","metadata":{"cellId":"p86wo7s8oulxttelj13jdn","trusted":true},"outputs":[],"execution_count":null}]}