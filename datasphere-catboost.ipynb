{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python"},"papermill":{"duration":392.708184,"end_time":"2020-11-23T16:52:33.291774","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-11-23T16:46:00.583590","version":"2.1.0"},"notebookId":"f600143a-7bc3-4fac-9a49-5e1bc0bfa96a"},"cells":[{"cell_type":"code","source":"#!M\n# !pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1\n# %pip install --upgrade pip\n# %pip install -r /home/jupyter/work/resources/riiidNew/requirements.txt --upgrade\n# %pip install scipy  --ignore-installed scipy --upgrade\nprint(\"ok\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":30.191831,"end_time":"2020-11-23T16:46:34.873724","exception":false,"start_time":"2020-11-23T16:46:04.681893","status":"completed"},"tags":[],"cellId":"r4gbiatk8h938d934p4aip","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!M\nimport numpy as np\nimport pandas as pd\nimport pyarrow\nfrom collections import defaultdict\n\nfrom catboost.utils import get_gpu_device_count\nfrom catboost import CatBoostClassifier, Pool\n\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n# import riiideducation\n# import torch\n# import pickle\nimport gc\nfrom pathlib import Path\nimport sys\n\nhomedir = Path.home()\nprint(str(homedir))\n\nsys.path.append(str(Path.cwd()/'riiidNew'))\nfrom config import config_datasphere\nconfig_datasphere(force=True)\n\n# Error handling, ignore all\n# np.seterr(divide = 'ignore', invalid = 'ignore')\n\nprint(f\"pyarrow {pyarrow.__version__}\")\nprint(f\"curdir {Path.cwd()}\")","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":2.195965,"end_time":"2020-11-23T16:46:37.127434","exception":false,"start_time":"2020-11-23T16:46:34.931469","status":"completed"},"tags":[],"cellId":"t3n208wi88qtuu9cu40m","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\nif str(Path.home()) == \"/root\":  # DATASPHERE\n    import sys\n    sys.path.append(str(Path.cwd()/'riiidNew'))\n    from pathlib import Path\n    from preprocess import preprocess_train_data\n    import config\n\nif str(homedir) == \"/home/sergey\":   # Home computer\n    kaggle_path = Path.cwd()/\"kaggle_tmp/\"\n    questions_df = pd.read_csv('/mnt/data30G/2020riiid/questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n    # train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns = set(dtypes.keys())).to_pandas()\n    # train_df = dt.fread('/mnt/data30G/2020riid/train.csv', columns = set(dtypes.keys())).to_pandas()\n    test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid_1e4.feather\", columns=dtypes.keys())\n    train_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train_1e5.feather\", columns=dtypes.keys())\n    # test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid_1e4.feather\", columns=dtypes.keys())\n    # train_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train.feather\", columns=dtypes.keys())\n    # test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid.feather\", columns=dtypes.keys())\n\nelif str(homedir) == \"/root\":   # Datasphere\n    \n    path = Path.cwd()/\"riiidNew\"/\"data1\"\n    kaggle_path = Path.cwd()/\"riiidNew\"/\"kaggle\"\n    questions_df = pd.read_csv(path/'questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n    train_df = pd.read_feather(path/\"train.feather\", columns=config.dtypes.keys())\n    # test_df = pd.read_feather(path/\"cv1_valid.feather\", columns=dtypes.keys())\n    # train_df = pd.read_pickle(path/'cv1_train.pickle.zip').astype(config.dtypes, errors=\"ignore\")\n    train_df = train_df.iloc[:int(1e4)]\n    # test_df = pd.read_pickle(path/\"cv1_valid.pickle.zip\").astype(config.dtypes, errors=\"ignore\")\n    # test_df = pd.read_pickle(path/\"cv1_valid.pickle.zip\").astype(config.dtypes, errors=\"ignore\")\n    # test_df = pd.read_pickle(path/\"cv1_valid.pickle.zip\").astype(dtypes, errors=\"ignore\")\n    # test_df = test_df.iloc[:int(1e4)]\n\n# print(f\"train_df shape = {train_df.shape}\")\ntrain_user_agg, train_content_agg, train_df = preprocess_train_data(train_df, questions_df, config.target, config.dtypes)\n# test_user_agg, test_content_agg, test_df = preprocess_train_data(test_df, questions_df, target, dtypes)\nprint(\"preprocess ok\")\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')\n\n# features = [\n#     'content_id', 'prior_question_elapsed_time',\n#     'prior_question_had_explanation', 'user_correctness',\n#     'part', 'content_count'\n# ]\n\n# cat_features = [\n#     'prior_question_had_explanation',\n#     'part',\n# ]\n\n# Catboost initial parameters, to overload next by pds for bayesian search\n# prior_params = {\n#     'loss_function': 'Logloss',\n#     'eval_metric': 'AUC',\n#     'custom_metric': 'AUC:hints=skip_train~false',\n\n#     'task_type': 'GPU' if str(homedir) == \"/root\" else 'CPU',\n#     # 'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n#     'grow_policy': 'Lossguide',\n#     'iterations': 2,\n#     'learning_rate': 4e-3,\n#     'random_seed': 0,\n#     'l2_leaf_reg': 5e-1,\n#     'depth': 10,\n#     'max_leaves': 10,\n#     'border_count': 128,\n#     'verbose': 150,\n#     'od_type': 'Iter',\n#     'od_wait': 50,\n# }\n\n# catboost params intervals for bayesian search\n# pds = {\n#     'l2_leaf_reg': [2e-1, 5e2],\n#     'depth': [8, 25],\n#     'max_leaves': [10, 150],\n#     'border_count': [50, 300],\n\n# }\n\n# Parameters dtypes to adjust with catboost\n# pds_dtypes = {\n#     'iterations': int,\n#     'l2_leaf_reg': float,\n#     'depth': int,\n#     'max_leaves': int,\n#     'border_count': int,\n\n# }\n\n# Training and validating data\ntrain_set = Pool(train_df[config.features], label = train_df[config.target], cat_features=config.cat_features)\n# val_set = Pool(test_df[features], label = test_df[target])\n\n\nprint(\"optimizer_maxCV ..\")\noptimizer_maxCV = bayesian_catboost_searchCV(\n    train_set,\n    prior_params=config.prior_params,\n    pds=config.pds, pds_dtypes=config.pds_dtypes,\n    init_points=5, n_iter=7, verbose=False,\n)\n\nprint(optimizer_maxCV)\n\npds_fitted = {key: config.pds_dtypes[key](val) for key, val in optimizer_maxCV[\"params\"].items()}\nparams = prior_params.copy()\nparams.update(pds_fitted)\nprint(\"params\")\nprint(params)\n\n# Ratio is 6/24 = 25%\n# valid_df = train_df.groupby('user_id').tail(6)\n# train_df.drop(valid_df.index, inplace = True)\n\n# val_set = Pool(valid_df[features], label = valid_df[target])\n\n# Model definition\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set)\nprint(f\"kaggle_path {kaggle_path}\")\n\nmodel.save_model(f\"{kaggle_path/'catboost.model'}\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"x457vefy7xyyyibor5o8f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n{'loss_function': 'Logloss', 'eval_metric': 'AUC', 'task_type': 'GPU', 'grow_policy': 'Lossguide', 'iterations': 15000, 'learning_rate': 0.01, 'random_seed': 0, 'l2_leaf_reg': 0.1, 'depth': 8, 'border_count': 128, 'verbose': 150, 'od_type': 'Iter', 'od_wait': 50}\nbestTest = 0.7371392846\n\nbestTest = 0.7370638549\nbestTest = 0.7364509702\n","metadata":{"cellId":"nl7v0tm6byjfdfngg11ydh"}},{"cell_type":"code","source":"init_datasphere()\n\nmodel.get_best_score()","metadata":{"cellId":"c1pc7wh24o8ipu8gk23jbh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exit(1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"5k037w11a3xjpxkmmu2le8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.051177,"end_time":"2020-11-23T16:52:31.451640","exception":false,"start_time":"2020-11-23T16:52:31.400463","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"ba4wdwyypnjxlgzfd769b9"}},{"cell_type":"code","source":"#!M\nuser_sum_dict = train_user_agg['sum'].astype('int32').to_dict(defaultdict(int))\nuser_count_dict = train_user_agg['count'].astype('int32').to_dict(defaultdict(int))\ncontent_sum_dict = train_content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = train_content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\nfor filename, dic in zip([\"user_sum_dict\", \"user_count_dict\", \"content_sum_dict\", \"content_count_dict\"],\n                         [user_sum_dict, user_count_dict, content_sum_dict, content_count_dict]):\n    with open(f'{kaggle_path}/{filename}.pickle', 'wb') as handle:\n        pickle.dump(dic, handle)\n","metadata":{"papermill":{"duration":0.618845,"end_time":"2020-11-23T16:52:32.122494","exception":false,"start_time":"2020-11-23T16:52:31.503649","status":"completed"},"tags":[],"cellId":"7ui7ur3vc7044m7d88w7h2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()\ndel train_df\ngc.collect()\n","metadata":{"cellId":"t8443l740slicr5x1cuvl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nvalidaten_flg = True\nif validaten_flg:\n    from emulator import Iter_Valid\n    iter_test = Iter_Valid(test_df,max_user=1000)\n    predicted = []\n    def set_predict(df):\n        predicted.append(df)\nelse:\n    import riiideducation\n    env = riiideducation.make_env()\n    iter_test = env.iter_test()\n    set_predict = env.predict","metadata":{"cellId":"6qrpfrl3bhn281e5c5twea","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cumcount = sum([len(df) for df in predicted])\n# count = 0\n# pbar = tqdm(total=cumcount)\n# previous_test_df = None\n# for (current_test, current_prediction_df) in iter_test:\n#     count+=1\n#     if previous_test_df is not None:\n#         answers = eval(current_test[\"prior_group_answers_correct\"].iloc[0])\n#         responses = eval(current_test[\"prior_group_responses\"].iloc[0])\n#         previous_test_df['answered_correctly'] = answers\n#         previous_test_df['user_answer'] = responses\n#         # your feature extraction and model training code here\n#     previous_test_df = current_test.copy()\n#     current_test = current_test[current_test.content_type_id == 0]\n#     # your prediction code here\n#     current_test['answered_correctly'] = model.predict(current_test[features])  # 0.5\n#     set_predict(current_test.loc[:,['row_id', 'answered_correctly']])\n#     pbar.update(len(current_test))\n# print(f\"count {count} {len(predicted)}\")","metadata":{"papermill":{"duration":466.435487,"end_time":"2020-11-13T14:06:20.068150","exception":false,"start_time":"2020-11-13T13:58:33.632663","status":"completed"},"tags":[],"cellId":"hkq3ce6f9zot3ymxi9x1s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprior_test_df = None\nfor (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n\n        # for user_id, content_id, answered_correctly in prior_test_df[[\"user_id\", \"content_id\", target]].values:\n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n    test_df.drop(labels=\"part\", axis=1, inplace=True)\n    test_df.content_id = test_df.content_id.astype(int)\n    \n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')    \n    user_sum = np.zeros(len(test_df), dtype = np.int32)\n    user_count = np.zeros(len(test_df), dtype = np.int32)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n\n    test_df['user_correctness'] = user_sum / user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum / content_count\n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    set_predict(test_df[['row_id', target]])","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"yzork8xo17phg263eq5lvj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!M\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"r0wtm7br3uocd9325isg1s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!M\n","metadata":{"cellId":"p86wo7s8oulxttelj13jdn","trusted":true},"outputs":[],"execution_count":null}]}