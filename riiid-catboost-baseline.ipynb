{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python"},"papermill":{"duration":392.708184,"end_time":"2020-11-23T16:52:33.291774","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-11-23T16:46:00.583590","version":"2.1.0"},"notebookId":"ee969155-d4d7-4a0b-aaac-221773547634"},"cells":[{"cell_type":"markdown","source":"# Use the package 'datatable' for fast handling","metadata":{"papermill":{"duration":0.022505,"end_time":"2020-11-23T16:46:04.662261","exception":false,"start_time":"2020-11-23T16:46:04.639756","status":"completed"},"tags":[],"cellId":"3ljn02n8bx95hgtho7zrx"}},{"cell_type":"code","source":"# !pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1\n# %pip install --upgrade pip\n%pip install -r /home/jupyter/work/resources/riiidNew/requirements.txt --upgrade \n# %pip install --upgrade wheel\n# %pip install --upgrade pyarrow==2.0.0\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":30.191831,"end_time":"2020-11-23T16:46:34.873724","exception":false,"start_time":"2020-11-23T16:46:04.681893","status":"completed"},"tags":[],"cellId":"r4gbiatk8h938d934p4aip","trusted":true},"outputs":[{"name":"stdout","text":"\u001B[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001B[0m\nCollecting catboost\n  Using cached catboost-0.24.3-cp37-none-manylinux1_x86_64.whl (66.1 MB)\nCollecting dask\n  Using cached dask-2.30.0-py3-none-any.whl (848 kB)\nCollecting lightgbm\n  Using cached lightgbm-3.1.0-py2.py3-none-manylinux1_x86_64.whl (1.8 MB)\nCollecting matplotlib\n  Using cached matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\nCollecting numpy\n  Using cached numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\nCollecting pandas\n  Using cached pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\nCollecting Pillow==6.2.2\n  Using cached Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\nCollecting plotly\n  Using cached plotly-4.14.0-py2.py3-none-any.whl (13.2 MB)\nCollecting seaborn\n  Using cached seaborn-0.11.0-py3-none-any.whl (283 kB)\nCollecting torch==1.6.0\n  Using cached torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\nCollecting tqdm==4.53\n  Using cached tqdm-4.53.0-py2.py3-none-any.whl (70 kB)\nCollecting wheel\n  Using cached wheel-0.36.1-py2.py3-none-any.whl (34 kB)\nCollecting cycler>=0.10\n  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\nCollecting future\n  Using cached future-0.18.2-py3-none-any.whl\nCollecting graphviz\n  Using cached graphviz-0.15-py2.py3-none-any.whl (18 kB)\nCollecting kiwisolver>=1.0.1\n  Using cached kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\nCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\nCollecting python-dateutil>=2.1\n  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\nCollecting pytz>=2017.2\n  Using cached pytz-2020.4-py2.py3-none-any.whl (509 kB)\nCollecting pyyaml\n  Using cached PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl\nCollecting retrying>=1.3.3\n  Using cached retrying-1.3.3-py3-none-any.whl\nCollecting scikit-learn!=0.22.0\n  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\nCollecting joblib>=0.11\n  Using cached joblib-0.17.0-py3-none-any.whl (301 kB)\nCollecting scipy\n  Using cached scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\nCollecting six\n  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\nCollecting threadpoolctl>=2.0.0\n  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\nInstalling collected packages: six, numpy, threadpoolctl, scipy, retrying, pytz, python-dateutil, pyparsing, Pillow, kiwisolver, joblib, cycler, scikit-learn, pyyaml, plotly, pandas, matplotlib, graphviz, future, wheel, tqdm, torch, seaborn, lightgbm, dask, catboost\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Necessary packages","metadata":{"papermill":{"duration":0.018571,"end_time":"2020-11-23T16:46:34.912747","exception":false,"start_time":"2020-11-23T16:46:34.894176","status":"completed"},"tags":[],"cellId":"pz3uk46r8c9ejefq2dp3m"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pyarrow\nfrom collections import defaultdict\n# import datatable as dt\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n# import riiideducation\nimport torch\nimport pickle\nimport gc\nfrom pathlib import Path\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":2.195965,"end_time":"2020-11-23T16:46:37.127434","exception":false,"start_time":"2020-11-23T16:46:34.931469","status":"completed"},"tags":[],"cellId":"t3n208wi88qtuu9cu40m","trusted":true},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"print(f\"pyarrow {pyarrow.__version__}\")","metadata":{"cellId":"8hrdxo2qiag9bfjnytsaz","trusted":true},"outputs":[{"name":"stdout","text":"pyarrow 0.17.1\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"kaggle_path = \"kaggle/\"\ncurdir = Path.cwd()\nhomedir = Path.home()\nprint(f\"curdir =  {curdir}\")\n\nprint(f\"homedir =  {homedir}\")","metadata":{"pycharm":{"name":"#%%\n"},"cellId":"rtzf64uywddhqefszjout","trusted":true},"outputs":[{"name":"stdout","text":"curdir =  /home/jupyter/work/resources\nhomedir =  /root\n","output_type":"stream"}],"execution_count":104},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.019813,"end_time":"2020-11-23T16:46:37.168146","exception":false,"start_time":"2020-11-23T16:46:37.148333","status":"completed"},"tags":[],"cellId":"anxc5qpofibelpn72czs7"}},{"cell_type":"markdown","source":"* Data config","metadata":{"papermill":{"duration":0.019374,"end_time":"2020-11-23T16:46:37.206927","exception":false,"start_time":"2020-11-23T16:46:37.187553","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"9z1ri5b2lwike4azllkjpp"}},{"cell_type":"code","source":"dtypes = {\n    # 'row_id': 'int64',\n    'timestamp': 'int64',\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'content_type_id': 'int8',\n    'task_container_id': 'int16',\n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool',\n    'user_answer': 'int8',\n}\n\ntarget = 'answered_correctly'","metadata":{"papermill":{"duration":0.026993,"end_time":"2020-11-23T16:46:37.253096","exception":false,"start_time":"2020-11-23T16:46:37.226103","status":"completed"},"tags":[],"pycharm":{"name":"#%%\n"},"cellId":"4dtjg452dd7ekvta7bvj","trusted":true},"outputs":[],"execution_count":105},{"cell_type":"markdown","source":"* Import data","metadata":{"papermill":{"duration":0.018939,"end_time":"2020-11-23T16:46:37.290855","exception":false,"start_time":"2020-11-23T16:46:37.271916","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"3a2ug3cpyyucna2e9lpxs"}},{"cell_type":"code","source":"%%time\n\n\nif str(homedir) == \"/home/sergey\":   # Home computer\n    questions_df = pd.read_csv('/mnt/data30G/2020riid/questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n    # train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns = set(data_types_dict.keys())).to_pandas()\n    # train_df = dt.fread('/mnt/data30G/2020riid/train.csv', columns = set(data_types_dict.keys())).to_pandas()\n    # test_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid_1e4.feather\", columns=data_types_dict.keys())\n    # train_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train_1e5.feather\", columns=data_types_dict.keys())\n    # valid_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid_1e4.feather\", columns=data_types_dict.keys())\n    # train_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train.feather\", columns=data_types_dict.keys())\n    # valid_df = pd.read_feather(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_valid.feather\", columns=data_types_dict.keys())\n\nelif str(homedir) == \"/root\":   # Datasphere\n    path = Path(\"/home/jupyter/work/resources/riiidNew/data\")\n    questions_df = pd.read_csv(path/'questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n#     train_df = pd.read_feather(path/\"cv1_train_1e5.feather\", columns=data_types_dict.keys())\n#     valid_df = pd.read_feather(path/\"cv1_valid_1e4.feather\", columns=data_types_dict.keys())\n    # valid_df = pd.read_feather(path/\"data/cv1_valid_1e4.feather\", columns=data_types_dict.keys())\n    # train_df = pd.read_feather(path/\"cv1_train.feather\", columns=data_types_dict.keys())\n    # valid_df = pd.read_feather(path/\"cv1_valid.feather\", columns=data_types_dict.keys())\n    train_df = pd.read_pickle(path/\"cv1_train.pickle.zip\")\n    valid_df = pd.read_pickle(path/\"cv1_valid.pickle.zip\")\n    train_df.dtype(dtypes)\n    valid_df.dtype(dtypes)\n    # , columns=data_types_dict.keys()\n","metadata":{"papermill":{"duration":65.145462,"end_time":"2020-11-23T16:47:42.455648","exception":false,"start_time":"2020-11-23T16:46:37.310186","status":"completed"},"tags":[],"pycharm":{"name":"#%%\n"},"cellId":"67ntodpzk98fzmgeq6pe79","trusted":true},"outputs":[{"traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)","\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[0;32m/home/jupyter/work/pyenv/pandas/io/feather_format.py\u001B[0m in \u001B[0;36mread_feather\u001B[0;34m(path, columns, use_threads)\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshould_close\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_filepath_or_buffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfeather\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_feather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muse_threads\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muse_threads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[0;31m# s3fs only validates the credentials when the file is closed.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/feather.py\u001B[0m in \u001B[0;36mread_feather\u001B[0;34m(source, columns, use_threads, memory_map)\u001B[0m\n\u001B[1;32m    212\u001B[0m     \"\"\"\n\u001B[1;32m    213\u001B[0m     \u001B[0m_check_pandas_version\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 214\u001B[0;31m     return (read_table(source, columns=columns, memory_map=memory_map)\n\u001B[0m\u001B[1;32m    215\u001B[0m             .to_pandas(use_threads=use_threads))\n\u001B[1;32m    216\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/feather.py\u001B[0m in \u001B[0;36mread_table\u001B[0;34m(source, columns, memory_map)\u001B[0m\n\u001B[1;32m    243\u001B[0m         \u001B[0mtable\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_indices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mt\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_types\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 245\u001B[0;31m         \u001B[0mtable\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_names\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    246\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m         \u001B[0mcolumn_type_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcolumn_types\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/feather.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.FeatherReader.read_names\u001B[0;34m()\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/public-api.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.pyarrow_wrap_table\u001B[0;34m()\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n","\u001B[0;31mArrowInvalid\u001B[0m: Column 0: In chunk 0: Invalid: Buffer #1 too small in array of type int64 and length 65536: expected at least 524288 byte(s), got 288379"],"ename":"ArrowInvalid","evalue":"Column 0: In chunk 0: Invalid: Buffer #1 too small in array of type int64 and length 65536: expected at least 524288 byte(s), got 288379","output_type":"error"}],"execution_count":106},{"cell_type":"markdown","source":"* Information of the training dataset","metadata":{"papermill":{"duration":0.020594,"end_time":"2020-11-23T16:47:42.496580","exception":false,"start_time":"2020-11-23T16:47:42.475986","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"vk8q857fkbl0ou00kw2wqoe"}},{"cell_type":"code","source":"sep = '*' * 50\nprint(f'Training dataset detailed information \\n{sep}')\nprint(f'Columns: {train_df.columns} \\n{sep}')\nprint(f'Shape: {train_df.shape} \\n{sep}')\nprint(f'NA values in each column: {sum(train_df.isna().sum())} \\n{sep}')\n","metadata":{"papermill":{"duration":11.548951,"end_time":"2020-11-23T16:47:54.068264","exception":false,"start_time":"2020-11-23T16:47:42.519313","status":"completed"},"tags":[],"cellId":"x4nexxr3kgd9p8ff3m8it","trusted":true},"outputs":[{"name":"stdout","text":"Training dataset detailed information \n**************************************************\n","output_type":"stream"},{"traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","\u001B[0;32m<ipython-input-10-f919543d16dd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0msep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'*'\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m50\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Training dataset detailed information \\n{sep}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Columns: {train_df.columns} \\n{sep}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Shape: {train_df.shape} \\n{sep}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'NA values in each column: {sum(train_df.isna().sum())} \\n{sep}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mNameError\u001B[0m: name 'train_df' is not defined"],"ename":"NameError","evalue":"name 'train_df' is not defined","output_type":"error"}],"execution_count":41},{"cell_type":"code","source":"def prep_data(df, questions_df=questions_df):\n    # Exclude lectures\n    df = df[df[target] != -1].reset_index(drop = True, inplace = False)\n    # Fill NaN values in the 'prior_question_had_explanation' columns\n    df['prior_question_had_explanation'].fillna(False, inplace = True)\n    # Set type\n    df = df.astype(data_types_dict)\n    \n    # Answer for the previous questions of users\n    df['lag'] = df.groupby('user_id')[target].shift()\n    # For each user (groupby('user_id')), compute the cummulative number of correct answers and number answers in general\n    groupby = df.groupby('user_id')['lag']\n    cum = groupby.agg(['cumsum', 'cumcount'])\n\n    # User correctness (measure the users' learning progress)\n    df['user_correctness'] = cum['cumsum'] / cum['cumcount']\n    # Drop the 'lag' feature\n    df.drop(columns = ['lag'], inplace = True)\n    df.head()    \n    \n    # Overall correctness of users\n    user_agg = df.groupby('user_id')[target].agg(['sum', 'count'])\n    # Overall difficulty of questions\n    content_agg = df.groupby('content_id')[target].agg(['sum', 'count'])    \n\n    # Take only 24 last observations of each user\n    df = df.groupby('user_id').tail(24).reset_index(drop = True)\n    \n    df = pd.merge(df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    df.drop(columns = ['question_id'], inplace = True)\n\n    # How many questions have been answered in each content ID?\n    df['content_count'] = df['content_id'].map(content_agg['count']).astype('int32')\n    # How hard are questions in each content ID?\n    df['content_id'] = df['content_id'].map(content_agg['sum'] / content_agg['count'])\n    \n    return user_agg, content_agg, df\n    ","metadata":{"pycharm":{"name":"#%%\n"},"cellId":"4wg4fgp8nr8n4akqm0u61"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract the validation set","metadata":{"papermill":{"duration":0.021183,"end_time":"2020-11-23T16:48:53.873557","exception":false,"start_time":"2020-11-23T16:48:53.852374","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"hvfpq2wdqaf1hr8xwrqf4q"}},{"cell_type":"code","source":"train_user_agg, train_content_agg, train_df = prep_data(train_df)\n\nuser_sum_dict = train_user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = train_user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = train_content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = train_content_agg['count'].astype('int32').to_dict(defaultdict(int))\ngc.collect()","metadata":{"papermill":{"duration":1.524016,"end_time":"2020-11-23T16:48:55.418847","exception":false,"start_time":"2020-11-23T16:48:53.894831","status":"completed"},"tags":[],"cellId":"r2n4048i8x8wkxqr1vkg4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"mnk3tvhr8xflb1b4r082j"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_user_agg, valid_content_agg, valid_df = prep_data(valid_df)\ngc.collect()","metadata":{"papermill":{"duration":1.524016,"end_time":"2020-11-23T16:48:55.418847","exception":false,"start_time":"2020-11-23T16:48:53.894831","status":"completed"},"tags":[],"pycharm":{"name":"#%%\n"},"cellId":"m4theb47y267g8ssgpa7u"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.296199,"end_time":"2020-11-23T16:48:55.736885","exception":false,"start_time":"2020-11-23T16:48:55.440686","status":"completed"},"tags":[],"pycharm":{"name":"#%%\n"},"cellId":"4oc9zbvu5svofth6jffqls"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ratio is 6/24 = 25%\n# valid_df = train_df.groupby('user_id').tail(6)\n# train_df.drop(valid_df.index, inplace = True)","metadata":{"papermill":{"duration":3.111291,"end_time":"2020-11-23T16:48:58.928890","exception":false,"start_time":"2020-11-23T16:48:55.817599","status":"completed"},"tags":[],"cellId":"wihq857gcng65klgbn89e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.023143,"end_time":"2020-11-23T16:48:58.975838","exception":false,"start_time":"2020-11-23T16:48:58.952695","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"suwfns3lzdchqhn90tmrsi"}},{"cell_type":"markdown","source":"* Construct data","metadata":{"papermill":{"duration":0.021554,"end_time":"2020-11-23T16:48:59.019149","exception":false,"start_time":"2020-11-23T16:48:58.997595","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"btj0scm1va9c5b8kid5ow"}},{"cell_type":"code","source":"features = ['content_id', 'prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', \n            'part', 'content_count']\n\nparams = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'grow_policy': 'Lossguide',\n    'iterations': 25,\n    'learning_rate': 4e-3,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 6,\n    # 'max_leaves': 10,\n    'border_count': 128,\n    'verbose': 50,\n    'od_type': 'Iter',\n    'od_wait': 30,\n}","metadata":{"papermill":{"duration":0.431014,"end_time":"2020-11-23T16:48:59.471718","exception":false,"start_time":"2020-11-23T16:48:59.040704","status":"completed"},"tags":[],"pycharm":{"name":"#%%\n"},"cellId":"deuvgp09gf4q8shcxcfxeq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\n\n# Training and validating data\ntrain_set = Pool(train_df[features], label = train_df[target])\nval_set = Pool(valid_df[features], label = valid_df[target])","metadata":{"papermill":{"duration":4.226965,"end_time":"2020-11-23T16:49:03.721844","exception":false,"start_time":"2020-11-23T16:48:59.494879","status":"completed"},"tags":[],"cellId":"5w20q7e6r68iplz1whchn"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model definition\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set, eval_set = val_set, use_best_model = True)\n\nmodel.save_model(f\"{kaggle_path}catboost.model\")","metadata":{"papermill":{"duration":207.603145,"end_time":"2020-11-23T16:52:31.347675","exception":false,"start_time":"2020-11-23T16:49:03.744530","status":"completed"},"tags":[],"cellId":"rwobg1zoeqq7slz9m5n2ho"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.051177,"end_time":"2020-11-23T16:52:31.451640","exception":false,"start_time":"2020-11-23T16:52:31.400463","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"},"cellId":"ba4wdwyypnjxlgzfd769b9"}},{"cell_type":"code","source":"\nuser_sum_dict = train_user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = train_user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = train_content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = train_content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\nfor filename, dic in zip([\"user_sum_dict\", \"user_count_dict\", \"content_sum_dict\", \"content_count_dict\"],\n                         [user_sum_dict, user_count_dict, content_sum_dict, content_count_dict]):\n    with open(f'{kaggle_path}{filename}.pickle', 'wb') as handle:\n        pickle.dump(dic, handle)\n","metadata":{"papermill":{"duration":0.618845,"end_time":"2020-11-23T16:52:32.122494","exception":false,"start_time":"2020-11-23T16:52:31.503649","status":"completed"},"tags":[],"cellId":"7ui7ur3vc7044m7d88w7h2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()\n","metadata":{"cellId":"t8443l740slicr5x1cuvl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train_df\ngc.collect()\n","metadata":{"cellId":"10lykr4g0xjkfq9lik4q3tb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_df.head()","metadata":{"cellId":"5khjipzjrxcyojdlju0z"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# test_df = pd.read_pickle(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata/cv1_train.pickle.zip\")\n# test_df = test_df.iloc[:int(1e5)]","metadata":{"cellId":"2u9rdo546tnw42i6zuf969"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nvalidaten_flg = True\nif validaten_flg:\n    from emulator import Iter_Valid\n    iter_test = Iter_Valid(valid_df,max_user=1000)\n    predicted = []\n    def set_predict(df):\n        predicted.append(df)\nelse:\n    import riiideducation\n    env = riiideducation.make_env()\n    iter_test = env.iter_test()\n    set_predict = env.predict","metadata":{"cellId":"6qrpfrl3bhn281e5c5twea"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cumcount = sum([len(df) for df in predicted])\n# count = 0\n# pbar = tqdm(total=cumcount)\n# previous_test_df = None\n# for (current_test, current_prediction_df) in iter_test:\n#     count+=1\n#     if previous_test_df is not None:\n#         answers = eval(current_test[\"prior_group_answers_correct\"].iloc[0])\n#         responses = eval(current_test[\"prior_group_responses\"].iloc[0])\n#         previous_test_df['answered_correctly'] = answers\n#         previous_test_df['user_answer'] = responses\n#         # your feature extraction and model training code here\n#     previous_test_df = current_test.copy()\n#     current_test = current_test[current_test.content_type_id == 0]\n#     # your prediction code here\n#     current_test['answered_correctly'] = model.predict(current_test[features])  # 0.5\n#     set_predict(current_test.loc[:,['row_id', 'answered_correctly']])\n#     pbar.update(len(current_test))\n# print(f\"count {count} {len(predicted)}\")","metadata":{"papermill":{"duration":466.435487,"end_time":"2020-11-13T14:06:20.068150","exception":false,"start_time":"2020-11-13T13:58:33.632663","status":"completed"},"tags":[],"cellId":"hkq3ce6f9zot3ymxi9x1s"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_df","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"yzork8xo17phg263eq5lvj"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprior_test_df = None\nfor (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n    test_df.drop(labels=\"part\", axis=1, inplace=True)\n    test_df.content_id = test_df.content_id.astype(int)\n    \n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')    \n    user_sum = np.zeros(len(test_df), dtype = np.int16)\n    user_count = np.zeros(len(test_df), dtype = np.int16)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n\n    test_df['user_correctness'] = user_sum / user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum / content_count\n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    set_predict(test_df[['row_id', target]])","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"cellId":"t68i0opvn39cf0je5yqwh"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"yz96kwti5mshv7pk4co956"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprior_test_df = None\nfor (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n    test_df.drop(labels=\"part\", axis=1, inplace=True)\n    test_df.content_id = test_df.content_id.astype(int)\n    \n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')    \n    user_sum = np.zeros(len(test_df), dtype = np.int16)\n    user_count = np.zeros(len(test_df), dtype = np.int16)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n\n    test_df['user_correctness'] = user_sum / user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum / content_count\n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    set_predict(test_df[['row_id', target]])","metadata":{"papermill":{"duration":0.550421,"end_time":"2020-11-23T16:52:32.842277","exception":false,"start_time":"2020-11-23T16:52:32.291856","status":"completed"},"tags":[],"cellId":"ikplh5uhkz9cgxru9hxh"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"3k98g7y1jr5d2632d3lzyr"},"outputs":[],"execution_count":null}]}