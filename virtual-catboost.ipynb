{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#!M\n",
    "# !pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1\n",
    "# %pip install --upgrade pip\n",
    "# %pip install -r /home/jupyter/work/resources/riiidNew/requirements.txt --upgrade\n",
    "# %pip install scipy  --ignore-installed scipy --upgrade\n",
    "# %pip install pyarrow==0.17.1\n",
    "# %pip install bayesian-optimization\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/riiid/riiidNew\n"
     ]
    }
   ],
   "source": [
    "#!L\n",
    "# if str(Path.home()) == \"/root\":  # DATASPHERE\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import pyarrow\n",
    "from collections import defaultdict\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "sys.path.append(str(Path.cwd()/'riiidNew'))\n",
    "from preprocess import preprocess_train_data, divide_nan\n",
    "from catboost_bayesian_search import bayesian_catboost_search, bayesian_catboost_searchCV, param_adjust_dtypes\n",
    "import config\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape = (100000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Error handling, ignore all\n",
    "np.seterr(divide = 'ignore', invalid = 'ignore')\n",
    "\n",
    "\n",
    "if str(Path.home()) == \"/home/sergey\":   # Home computer\n",
    "    data_path = Path(\"/home/sergey/mnt/4.5Tb/Downloads/riiidCVdata\")\n",
    "    kaggle_path = Path.cwd()/\"kaggle_tmp\"\n",
    "    questions_df = pd.read_csv(data_path/'questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n",
    "    # test_df = pd.read_feather(data_path/\"cv1_valid_1e4.feather\", columns=config.dtypes.keys())\n",
    "    train_df = pd.read_feather(data_path/\"train_1e5.feather\", columns=config.dtypes.keys())\n",
    "    # test_df = pd.read_feather(data_path/\"cv1_valid_1e4.feather\", columns=config.dtypes.keys())\n",
    "    # train_df = pd.read_feather(data_path/\"cv1_train.feather\", columns=config.dtypes.keys())\n",
    "    # test_df = pd.read_feather(data_path/\"cv1_valid.feather\", columns=config.dtypes.keys())\n",
    "\n",
    "elif str(Path.home()) == \"/root\":   # DATASPHERE\n",
    "\n",
    "    data_path = Path.cwd()/\"riiidNew\"/\"data1\"\n",
    "    kaggle_path = Path.cwd()/\"riiidNew\"/\"kaggle\"\n",
    "    questions_df = pd.read_csv(data_path/'questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n",
    "    train_df = pd.read_feather(data_path/\"train.feather\", columns=config.dtypes.keys())\n",
    "    # train_df = pd.read_feather(data_path/\"train_5e6.feather\", columns=config.dtypes.keys())\n",
    "    # train_df = pd.read_feather(data_path/\"train_1e5.feather\", columns=config.dtypes.keys())\n",
    "\n",
    "elif str(Path.home()) == \"/home/riiid\":   # Virtual machine ycloud\n",
    "\n",
    "    data_path = Path.cwd()/\"data\"\n",
    "    kaggle_path = Path.cwd()/\"kaggle\"\n",
    "    questions_df = pd.read_csv(data_path/'questions.csv', usecols = [0, 3], dtype = {'question_id': 'int16', 'part': 'int8'})\n",
    "    # train_df = pd.read_feather(data_path/\"train.feather\", columns=config.dtypes.keys())\n",
    "    # train_df = pd.read_feather(data_path/\"train_5e6.feather\", columns=config.dtypes.keys())\n",
    "    # test_df = pd.read_feather(data_path/\"cv2_test.feather\", columns=config.dtypes.keys())\n",
    "    train_df = pd.read_feather(data_path/\"cv2_train_1e5.feather\", columns=config.dtypes.keys())\n",
    "    # train_df = pd.read_feather(data_path/\"train_1e5.feather\", columns=config.dtypes.keys())\n",
    "#     train_df = train_df.iloc[:100]\n",
    "\n",
    "print(f\"train_df shape = {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32933156</td>\n",
       "      <td>0</td>\n",
       "      <td>705741139</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32933157</td>\n",
       "      <td>20666</td>\n",
       "      <td>705741139</td>\n",
       "      <td>7860</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32933158</td>\n",
       "      <td>39172</td>\n",
       "      <td>705741139</td>\n",
       "      <td>7922</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32933159</td>\n",
       "      <td>58207</td>\n",
       "      <td>705741139</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32933160</td>\n",
       "      <td>75779</td>\n",
       "      <td>705741139</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  timestamp    user_id  content_id  content_type_id  \\\n",
       "0  32933156          0  705741139         128                0   \n",
       "1  32933157      20666  705741139        7860                0   \n",
       "2  32933158      39172  705741139        7922                0   \n",
       "3  32933159      58207  705741139         156                0   \n",
       "4  32933160      75779  705741139          51                0   \n",
       "\n",
       "   task_container_id  answered_correctly  prior_question_elapsed_time  \\\n",
       "0                  0                   1                          NaN   \n",
       "1                  1                   1                      16000.0   \n",
       "2                  2                   1                      19000.0   \n",
       "3                  3                   1                      17000.0   \n",
       "4                  4                   1                      17000.0   \n",
       "\n",
       "   prior_question_had_explanation  user_answer  \n",
       "0                            <NA>            0  \n",
       "1                           False            0  \n",
       "2                           False            1  \n",
       "3                           False            2  \n",
       "4                           False            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_function': 'Logloss', 'eval_metric': 'AUC', 'custom_metric': 'AUC:hints=skip_train~false', 'task_type': 'GPU', 'grow_policy': 'Lossguide', 'iterations': 2, 'thread_count': -1, 'learning_rate': 0.1, 'random_seed': 0, 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 0.1, 'depth': 10, 'max_leaves': 10, 'border_count': 128, 'verbose': 250, 'od_type': 'Iter', 'od_wait': 50}\n"
     ]
    }
   ],
   "source": [
    "print(config.prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 26\n",
    "model_name = \"model40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess for model fitting ok\n",
      "train_df.columns\n",
      "Index(['row_id', 'timestamp', 'user_id', 'content_id', 'content_type_id',\n",
      "       'task_container_id', 'answered_correctly',\n",
      "       'prior_question_elapsed_time', 'prior_question_had_explanation',\n",
      "       'user_answer', 'user_correctness', 'part', 'content_count',\n",
      "       'question_difficulty'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameGroupBy' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-822f46785fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#test_df = train_df.groupby('user_id').tail(horizon)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         raise AttributeError(\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{attr}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameGroupBy' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "# Obtain new train_df and validation test_df\n",
    "# train_df = pd.read_feather(path/\"cv2_train.feather\", columns=config.dtypes.keys())\n",
    "# train_df = pd.read_feather(data_path/\"train.feather\", columns=config.dtypes.keys())\n",
    "\n",
    "\n",
    "# Preprocess for model fitting\n",
    "train_user_agg, train_content_agg, train_df = preprocess_train_data(train_df, questions_df, config.target, config.dtypes)\n",
    "# test_user_agg, test_content_agg, test_df = preprocess_train_data(test_df, questions_df, config.target, config.dtypes)\n",
    "print(\"Preprocess for model fitting ok\")\n",
    "\n",
    "print(\"train_df.columns\")\n",
    "print(train_df.columns)\n",
    "# print(\"test_df.columns\")\n",
    "# print(test_df.columns)\n",
    "\n",
    "#test_df = train_df.groupby('user_id').tail(horizon)\n",
    "test_df = train_df.groupby('user_id').sample(horizon)\n",
    "train_df.drop(test_df.index, inplace = True)\n",
    "\n",
    "# Training and validating data\n",
    "train_set = Pool(train_df[config.features], label = train_df[config.target], cat_features=config.cat_features)\n",
    "print(\"train_set Pool ready\")\n",
    "\n",
    "val_set = Pool(test_df[config.features], label = test_df[config.target], cat_features=config.cat_features)\n",
    "\n",
    "print(\" val_set Pool ready\")\n",
    "# val_set = Pool(valid_df[features], label = valid_df[target])\n",
    "# params = param_adjust_dtypes(config.prior_params,\n",
    "#                              config.pds_dtypes,\n",
    "#                              optimizer_max[\"params\"])\n",
    "params = config.prior_params\n",
    "# params[\"learning_rate\"] = 3e-2\n",
    "# params['iterations'] = 25000\n",
    "\n",
    "print(\"params\")\n",
    "print(params)\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "# Fitting\n",
    "model.fit(train_set, eval_set = val_set, use_best_model = True)\n",
    "print(f\"kaggle_path {kaggle_path}\")\n",
    "\n",
    "model.save_model(f\"{kaggle_path}/catboost.{model_name}\")\n",
    "print(\"model.get_best_score()\")\n",
    "print(model.get_best_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'learn': {'Logloss': 0.5831425763098211, 'AUC': 0.7542276382446289}, 'validation': {'Logloss': 0.599964516913287, 'AUC': 0.7367952764034271}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path.home()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_sum_dict =  train_user_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "user_count_dict =  train_user_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "content_sum_dict =  train_content_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "content_count_dict =  train_content_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "\n",
    "for filename, dic in zip([\"user_sum_dict\", \"user_count_dict\", \"content_sum_dict\", \"content_count_dict\"],\n",
    "                         [user_sum_dict, user_count_dict, content_sum_dict, content_count_dict]):\n",
    "# for filename, dic in dict_.items():\n",
    "    with open(f'{kaggle_path}/{filename}.{model_name}.pickle', 'wb') as handle:\n",
    "        pickle.dump(dic, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "validaten_flg = True\n",
    "if validaten_flg:\n",
    "    from emulator import Iter_Valid\n",
    "    iter_test = Iter_Valid(test_df,max_user=1000)\n",
    "    predicted = []\n",
    "    def set_predict(df):\n",
    "        predicted.append(df)\n",
    "else:\n",
    "    import riiideducation\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    set_predict = env.predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prior_test_df = None\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    if prior_test_df is not None:\n",
    "        prior_test_df[config.target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n",
    "\n",
    "        # Exclude lectures\n",
    "        prior_test_df = prior_test_df[prior_test_df['content_type_id'] == 0].reset_index(drop = True)\n",
    "\n",
    "        prior_test_np = prior_test_df[[\"user_id\", \"content_id\", config.target]].values\n",
    "        for user_id, content_id, answered_correctly in prior_test_np:\n",
    "            user_sum_dict[user_id] += answered_correctly\n",
    "            user_count_dict[user_id] += 1\n",
    "            content_sum_dict[content_id] += answered_correctly\n",
    "            content_count_dict[content_id] += 1\n",
    "\n",
    "    prior_test_df = test_df.copy()\n",
    "    \n",
    "    # Exclude lectures\n",
    "    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n",
    "\n",
    "    test_df.drop(labels=\"part\", axis=1, inplace=True)\n",
    "    test_df.content_id = test_df.content_id.astype(int)\n",
    "    \n",
    "    test_df = pd.concat(\n",
    "        [test_df.reset_index(drop=True),\n",
    "         questions_df.reindex(test_df['content_id'].values).reset_index(drop=True)],\n",
    "         axis=1, sort=False, join=\"outer\")\n",
    "\n",
    "    # Fill NaN values in the 'prior_question_had_explanation' columns\n",
    "    test_df['prior_question_had_explanation'] = \\\n",
    "        test_df['prior_question_had_explanation'].fillna(False).astype('bool')\n",
    "\n",
    "\n",
    "    # test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    # user_sum = np.zeros(len(test_df), dtype = np.int32)\n",
    "    # user_count = np.zeros(len(test_df), dtype = np.int32)\n",
    "    # content_sum = np.zeros(len(test_df), dtype = np.int32)\n",
    "    # content_count = np.zeros(len(test_df), dtype = np.int32)\n",
    "\n",
    "    for col in config.new_features:\n",
    "        test_df[col] = 0\n",
    "\n",
    "    user_correctness_offset = test_df.columns.get_loc('user_correctness')\n",
    "    content_count_offset = test_df.columns.get_loc('content_count')\n",
    "    question_difficulty_offset = test_df.columns.get_loc('question_difficulty')\n",
    "\n",
    "\n",
    "    # Error! for i, (user_id, content_id) in prior_test_df[[\"user_id\", \"content_id\"]].iterrows():\n",
    "    # prior_test_np = prior_test_df[[\"user_id\", \"content_id\"]].values\n",
    "    # for i, (user_id, content_id) in enumerate(prior_test_np):\n",
    "    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n",
    "        # user_correctness\n",
    "        # user_correctness = np.nan if user_count_dict[user_id] == 0 else user_sum_dict[user_id] / user_count_dict[user_id]\n",
    "        test_df.iloc[i, user_correctness_offset] = divide_nan(user_sum_dict[user_id], user_count_dict[user_id])\n",
    "\n",
    "        # content_count\n",
    "        test_df.iloc[i, content_count_offset] = int(content_count_dict[content_id])\n",
    "\n",
    "        # content_id\n",
    "#         content_id_val = np.nan if content_count_dict[content_id] == 0 else content_sum_dict[content_id] / content_count_dict[content_id]\n",
    "        test_df.iloc[i, question_difficulty_offset] = divide_nan(content_sum_dict[content_id], content_count_dict[content_id])\n",
    "\n",
    "        # user_sum[i] = user_sum_dict[user_id]\n",
    "        # user_count[i] = user_count_dict[user_id]\n",
    "        # content_sum[i] = content_sum_dict[content_id]\n",
    "        # content_count[i] = content_count_dict[content_id]\n",
    "\n",
    "    # test_df['user_correctness'] = user_sum / user_count\n",
    "    # test_df['content_count'] = content_count.astype(int)\n",
    "    # test_df['question_difficulty'] = content_sum / content_count\n",
    "\n",
    "    test_df.prior_question_elapsed_time = test_df.prior_question_elapsed_time // 2000\n",
    "\n",
    "    test_df[config.target] = model.predict_proba(test_df[config.features])[:,1]\n",
    "    set_predict(test_df[['row_id', config.target]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
